# üìä Utiliser Spark via R

Dans le pr√©c√©dent chapitre, nous avons appris comment installer un client pour utiliser Spark, en mode Local ou en mode Cluster. Nous avons √©galement appris comment demander des ressources, et fermer notre session. Voyons maintenant comment utiliser ces outils afin d'effectuer des calculs. Il existe de nombreuses ressources, en particulier la [documentation officielle de Spark](https://spark.apache.org/docs/latest/quick-start.html). Nous allons ici voir des exemples fondamentaux :

* Comment ex√©cuter une requ√™te SQL
* Comment joindre deux tables avec Spark

Ces deux exemples seront trait√©s en SparklyR car ce package pr√©sente la m√™me syntaxe que deeplyr. Cependant, il est important de savoir que ce package est moins performant que d'utiliser SparkR. SparkR est pr√©f√©rable s'il est possible de l'utiliser.

Je ne pr√©senterai pas dans ce guide les utilisations avanc√©es de Spark (Gestion des graphs avec GraphX, Machine Learning avec MLlib ou encore le mode streaming). Ces utilisations sont tr√®s sp√©cifiques, mais font par ailleurs l'objet de documentation en ligne. Nous faisons mention de leur existence, car ces librairies sont tr√®s puissantes. Elles permettent des applications sur des volumes de donn√©es qui ne sont pas comparables avec les autres librairies disponibles sur le march√©.

## Faire une requ√™te SQL

Si vous d√©marrez un processus de conversion de code vers Spark, il est possible de ne pas imm√©diatement convertir l'ensemble du code. En effet, Spark est tout √† fait capable d'ex√©cuter du code SQL. Son moteur de calcul est moins optimis√© que lorsqu'il ex√©cute du code Spark. Cependant, cela reste une option int√©ressante en raison de son faible co√ªt de code, et de ses performances tout √† fait raisonnables.

Une fois l'environnement configur√© comme expliqu√© dans le chapitre pr√©c√©dent, ex√©cuter une requ√™te SQL s'effectue de la mani√®re suivante :

```r
# Charger une table au format parquet
spark_read_table(sc, "nom_table", source = "parquet", path = "/chemin/vers/le/fichier.parquet")

# Ex√©cuter une requ√™te SQL
result <- sparklyr::spark_session(sc) %>%
  sparklyr::invoke("sql", "SELECT * FROM nom_table WHERE colonne = 'valeur'")

# Afficher le r√©sultat
show(result)
spark_disconnect(sc)
```

Ici, j'ai choisi de faire une requ√™te SQL tr√®s simple, compos√©e d'un SELECT, un FROM et un WHERE. Je vais donc r√©cup√©rer toutes les colonnes de la table, mais en filtrant les lignes selon une colonne particuli√®re. J'ai affich√© le r√©sultat, et j'ai ferm√© la session Spark afin de lib√©rer les ressources.

## Effectuer une jointure simple

Dans cet exemple, deux tables seront cr√©√©es, mais on pourrait les charger depuis des fichiers dans des Spark DataFrames directement.

<table><thead><tr><th width="76" align="center">id</th><th width="261" align="center">valeur1</th><th width="67" align="center">id</th><th align="center">valeur2</th><th data-hidden></th></tr></thead><tbody><tr><td align="center">1</td><td align="center">A</td><td align="center">2</td><td align="center">X</td><td></td></tr><tr><td align="center">2</td><td align="center">B</td><td align="center">3</td><td align="center">Y</td><td></td></tr><tr><td align="center">3</td><td align="center">C</td><td align="center">1</td><td align="center">Z</td><td></td></tr></tbody></table>

On transforme les DataFrames cr√©√©s en Spark DataFrames avec les noms de colonnes appropri√©s. Enfin, on performe une jointure sur les deux tables.

<table><thead><tr><th width="73" align="center">id</th><th width="324" align="center">valeur1</th><th align="center">valeur2</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">A</td><td align="center">Z</td></tr><tr><td align="center">2</td><td align="center">B</td><td align="center">X</td></tr><tr><td align="center">3</td><td align="center">C</td><td align="center">Y</td></tr></tbody></table>

Voyons comment effectuer cette manipulation en SparklyR et en PySpark.

### SparklyR

```r
# Cr√©er deux data frames
df1 <- data.frame(id = c(1, 2, 3), value1 = c("A", "B", "C"))
df2 <- data.frame(id = c(1, 2, 3), value2 = c("Z", "X", "Y"))

# Convertir les data frames en DataFrames Spark
sdf1 <- sdf_copy_to(sc, df1, "df1")
sdf2 <- sdf_copy_to(sc, df2, "df2")

# Effectuer la jointure
result <- inner_join(sdf1, sdf2, by = "id")

# Afficher le r√©sultat
show(result)
spark_disconnect(sc)
```
