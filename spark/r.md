# üå† Utiliser Spark

Trois fa√ßons d'acc√©der √† Spark via R sont support√©es au CASD. Il est possible d'utiliser la librairie officiellement support√©e par Spark pour R : SparkR

Une alternative √† la syntaxe tr√®s proche de dplyr est √©galement disponible : Sparklyr.

Enfin, il est possible d'utiliser PySpark !

SparkR et PySpark sont les options les plus performantes et proposent une meilleure explication de ces traitements gr√¢ce √† [des DAG d√©taill√©s](dag/les-plans-dexecutions-et-catalyst.md).

Dans cet article, vous pouvez passer d'une syntaxe √† l'autre avec les onglets !

## Configurer Spark en mode local

Dans le pr√©c√©dent chapitre, nous avons appris comment [installer un client pour utiliser Spark](installer.md). Nous allons maintenant r√©server des ressources afin de pr√©parer Spark pour notre traitement. Voici la marche √† suivre pour se connecter en mode local :&#x20;

{% tabs %}
{% tab title="PySpark" %}
{% code overflow="wrap" %}
```python
from pyspark.sql import SparkSession

# Configuration des ressources
spark = SparkSession.builder \
    .appName("ExempleJointureSimple") \  # Nom de l'application Spark
    .config("spark.executor.memory", "8g") \  # M√©moire allou√©e par ex√©cuteur
    .config("spark.executor.cores", 4) \  # Coeurs par ex√©cuteur
    .config("spark.driver.memory", "4g") \  # M√©moire allou√©e au driver (processus principal)
    .config("spark.driver.cores", 2) \  # Coeurs allou√©s au driver
    .getOrCreate()
```
{% endcode %}
{% endtab %}

{% tab title="SparkR" %}
<pre class="language-r" data-overflow="wrap"><code class="lang-r"><strong>library(SparkR)
</strong>
# Initialisation d'une session Spark en mode local
sparkR.session(master = "local[*]", appName = "SparkR-Example")

# V√©rification de la session
sparkSession &#x3C;- sparkR.session()
</code></pre>
{% endtab %}

{% tab title="Sparklyr" %}
{% code overflow="wrap" %}
```r
library(sparklyr)

# Configuration des ressources
config <- spark_config()

config$spark.executor.memory <- "8g" # M√©moire allou√©e par ex√©cuteur
config$spark.executor.cores <- 4 # Coeurs par ex√©cuteur
config$spark.driver.memory <- "4g" # M√©moire allou√©e au driver
config$spark.driver.cores <- 2 # Coeurs allou√©s au driver

# Cr√©ation de la connexion Sparklyr
sc <- spark_connect(master = "local", config = config, version = "3.3.2")
```
{% endcode %}
{% endtab %}
{% endtabs %}

Voici un exemple simple de configuration Spark. Les ressources doivent √™tre proportionnelles √† la t√¢che effectu√©e et aux donn√©es trait√©es par votre application.

## Effectuer des traitements

Voyons d√©sormais comment utiliser ces outils pour effectuer des calculs. Il existe de nombreuses ressources, en particulier la [documentation officielle de Spark](https://spark.apache.org/docs/latest/quick-start.html). Nous allons ici voir des exemples fondamentaux :

* Comment ex√©cuter une requ√™te SQL
* Comment joindre deux tables avec Spark

Ces deux exemples seront trait√©s en SparklyR, car ce package pr√©sente la m√™me syntaxe que deeplyr. Les deux librairies SparkR et SparklyR permettent d'interagir avec Spark, avec des fonctionnalit√©s similaires. SparklyR est ind√©pendant tandis que SparkR est une librairie officielle de Spark (comme PySpark).

Je ne pr√©senterai pas dans ce guide les utilisations avanc√©es de Spark (Gestion des graphs avec GraphX, Machine Learning avec MLlib ou encore le mode streaming). Ces utilisations sont tr√®s sp√©cifiques, mais font par ailleurs l'objet de documentation en ligne. Je fais mention de leur existence, car ces librairies sont tr√®s puissantes. Elles permettent des applications sur des volumes de donn√©es qui ne sont pas comparables avec les autres librairies disponibles sur le march√©.

### Faire une requ√™te SQL

Si vous d√©marrez un processus de conversion de code vers Spark, il est possible de ne pas imm√©diatement convertir l'ensemble du code. En effet, Spark est tout √† fait capable d'ex√©cuter [du code SQL](../stockage/sql/bases.md). Son moteur de calcul est moins optimis√© que lorsqu'il ex√©cute du code Spark. Cependant, cela reste une option int√©ressante en raison de son faible co√ªt de code, et de ses performances tout √† fait raisonnables.

Une fois l'environnement configur√© comme expliqu√© dans le chapitre pr√©c√©dent, ex√©cuter une requ√™te SQL s'effectue de la mani√®re suivante :

{% tabs %}
{% tab title="PySpark" %}
<pre class="language-python" data-overflow="wrap"><code class="lang-python"><strong># Charger une table Spark depuis un fichier Parquet :
</strong>df = spark.read.parquet("/chemin/vers/France.parquet")
df.createOrReplaceTempView("France")

# Ex√©cution d'une requ√™te SQL sur la table
result = spark.sql("SELECT region, count(departement) FROM France GROUP BY region")

# Afficher le r√©sultat
result.show()
</code></pre>
{% endtab %}

{% tab title="SparkR" %}
```r
# Charger une table Spark depuis un fichier Parquet :
df <- read.parquet("/chemin/vers/France.parquet")
createOrReplaceTempView(df, "parquet_table")

# Ex√©cution d'une requ√™te SQL sur la table
result <- sql("SELECT region, count(departement) FROM France GROUP BY region")

# Affichage des r√©sultats
showDF(result)
```
{% endtab %}

{% tab title="Sparklyr" %}
{% code overflow="wrap" %}
```r
# Charger une table Spark depuis un fichier Parquet :
spark_read_table(sc, "France", source = "parquet", path = "/chemin/vers/France.parquet")

# Ex√©cution d'une requ√™te SQL sur la table
result <- sparklyr::spark_session(sc) %>%
  sparklyr::invoke("sql", "SELECT region, count(departement) FROM France GROUP BY region")

# Affichage des r√©sultats
show(result)
```
{% endcode %}
{% endtab %}
{% endtabs %}

J'ai ici imagin√© une table France, contenant les d√©partements et leur r√©gion d'appartenance. J'ai compt√© le nombre de d√©partements par r√©gion en utilisant un group by. De plus, il faut toujours fermer sa session avec spark.stop().

### Effectuer une jointure simple

Dans cet exemple, deux tables seront cr√©√©es, mais on pourrait les charger depuis des fichiers dans des Spark DataFrames directement.

<table><thead><tr><th width="76" align="center">id</th><th width="261" align="center">valeur1</th><th width="67" align="center">id</th><th align="center">valeur2</th><th data-hidden></th></tr></thead><tbody><tr><td align="center">1</td><td align="center">A</td><td align="center">2</td><td align="center">X</td><td></td></tr><tr><td align="center">2</td><td align="center">B</td><td align="center">3</td><td align="center">Y</td><td></td></tr><tr><td align="center">3</td><td align="center">C</td><td align="center">1</td><td align="center">Z</td><td></td></tr></tbody></table>

On transforme les DataFrames cr√©√©s en Spark DataFrames avec les noms de colonnes appropri√©s. Enfin, on performe une jointure sur les deux tables.

<table><thead><tr><th width="73" align="center">id</th><th width="324" align="center">valeur1</th><th align="center">valeur2</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">A</td><td align="center">Z</td></tr><tr><td align="center">2</td><td align="center">B</td><td align="center">X</td></tr><tr><td align="center">3</td><td align="center">C</td><td align="center">Y</td></tr></tbody></table>

Voyons comment effectuer cette manipulation :

{% tabs %}
{% tab title="PySpark" %}
```python
# Cr√©er deux DataFrames
data1 = [(1, "A"), (2, "B"), (3, "C")]
data2 = [(1, "Z"), (2, "X"), (3, "Y")]

columns1 = ["id", "valeur1"]
columns2 = ["id", "valeur2"]

df1 = spark.createDataFrame(data1, columns1)
df2 = spark.createDataFrame(data2, columns2)

# Effectuer la jointure
result = df1.join(df2, "id", "inner")

# Afficher le r√©sultat
result.show()
spark.stop()
```
{% endtab %}

{% tab title="SparkR" %}
```r
# Cr√©er deux DataFrames
data1 <- list(list(1, "A"), list(2, "B"), list(3, "C"))
data2 <- list(list(1, "Z"), list(2, "X"), list(3, "Y"))

columns1 <- c("id", "valeur1")
columns2 <- c("id", "valeur2")

df1 <- createDataFrame(data1, schema = columns1)
df2 <- createDataFrame(data2, schema = columns2)

# Effectuer la jointure
result <- join(df1, df2, df1$id == df2$id, "inner")

# Afficher le r√©sultat
showDF(result)
sparkR.session.stop()
```
{% endtab %}

{% tab title="Sparklyr" %}
```r
# Cr√©er deux data frames
df1 <- data.frame(id = c(1, 2, 3), value1 = c("A", "B", "C"))
df2 <- data.frame(id = c(1, 2, 3), value2 = c("Z", "X", "Y"))

# Convertir les data frames en DataFrames Spark
sdf1 <- sdf_copy_to(sc, df1, "df1")
sdf2 <- sdf_copy_to(sc, df2, "df2")

# Effectuer la jointure
result <- inner_join(sdf1, sdf2, by = "id")

# Afficher le r√©sultat
show(result)
spark_disconnect(sc)
```
{% endtab %}
{% endtabs %}
