---
layout:
  title:
    visible: true
  description:
    visible: true
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: false
---

# üöß Installer Spark

## Les √©l√©ments n√©cessaires

### Installer Spark

Pour simplifier l'installation de Spark, nous avons con√ßu un script qui effectue cette installation de fa√ßon automatis√©e. Il est situ√© dans le dossier Python du dossier Raccourcis de votre bureau. Que vous souhaitiez adresser le logiciel Spark avec une syntaxe proche de R ou de Python, cette √©tape est n√©cessaire.

Cliquez sur AutoInstallSpark.bat afin d'effectuer l'installation.

Vous devrez d'abord choisir une version de Spark parmi la liste de celle disponible en saisissant son num√©ro. Je vous conseille la plus r√©cente.&#x20;

**Attention :** Vous devez attendre la fin du script et qu'il demande d'appuyer sur entr√©e pour terminer. Il ne faut pas fermer la fen√™tre du terminal ouvert pendant le script, cette op√©ration n'est pas tr√®s longue. Il suffit d'attendre la copie de l'ensemble des fichiers.

Ce script d√©compresse l'archive contenant Spark, copie les fichiers n√©cessaires √† son utilisation sous Windows, et param√®tres les variables d'environnement.

Vous pouvez tester l'installation avec la commande suivante dans un environnement anaconda par exemple (Miniconda Prompt)¬†:

```bash
spark-shell
```

### Installer le package SparklyR pour travailler avec SparkR dans Rstudio

Si vous souhaitez utiliser SparkR pour soumettre vos instructions au cluster, vous aurez besoin de la librairie SparklyR. Voici la marche √† suivre une fois que l'installation de Spark a √©t√© effectu√©e :

Proc√©dez d'abord √† l'installation du package SparklyR √† l'aide du d√©p√¥t, comme dans les chapitres pr√©c√©dents, puis chargez-le¬†:

```r
install.packages("sparklyr")
library("sparklyr")
```

Ensuite, connectez-vous au cluster :

```r
sc <- spark_connect(master="local")
```

Vous poss√©dez un serveur Spark adressable avec R pour votre application‚ÄØ!

### Installer PySpark et adresser le cluster avec un Notebook Jupyter

PySpark peut √™tre appel√©  en choisissant un kernel adapt√© pour ex√©cuter votre notebook. Pour cela :&#x20;

Ouvrez un miniconda prompt dans la version de votre choix.&#x20;

Attention : il est n√©cessaire de poss√©der un environnement anaconda dans une version compatible avec la version de Spark install√©e. Par exemple, **pour Spark 3.3.2, Python 3.9 est adapt√©, mais pas 3.11.**&#x20;

```python
conda create --name nom_env python --offline
```

Lancez et gardez ouvert le script pip-install-package et saissisez, dans votre miniconda prompt :

```python
pip install pyspark==3.3.2
pip install ipykernel --user
```

La version de votre paquet python Pyspark doit √™tre identique √† la version de Spark install√©e sur votre espace ! Puisqu'il s'agit d'un nouvel environnement d√©di√© aux notebook, ipykernel est √©galement n√©c√©ssaire.

Vous pouvez maintenant choisir le kernel adapt√© dans Visual Studio Code. Attention √† choisir le bon kernel, et que l'environnement en question poss√®de le package ipykernel qui permet d'ex√©cuter des notebooks.

En cas d'erreur avec py4j, vous pouvez ajouter deux cellules en entr√©e de script (avant la cr√©ation de l'environnement Spark !) :&#x20;

```python
import os 
import sys

os.environ['PYTHON_SPARK'] = sys.executable
os.environ['PYTHON_DRIVER_SPARK'] = sys.executable
```

### Installer les autres d√©pendances de Spark

Les fichiers Jar additionnels dont vous pouvez avoir besoin pour installer des librairies Spark peuvent √™tre obtenus dans le m√™me dossier que pour l'installation de SparklyR : dans S:\Spark\jar\_files. Vous trouverez ici les binaires √† copier, par exemple, celui qui permet d'ouvrir un fichier SAS avec Spark : spark-sas7bdat, dans diff√©rentes versions. De m√™me, vous pourrez installer parso avec la man≈ìuvre qui suit :

* Copiez le Jar de votre choix dans le dossier : "C:\Users\projet\_0\_p\_nom0000\AppData\Local\spark\spark-X.X.X-bin-hadoopX\jars"

