---
layout:
  title:
    visible: true
  description:
    visible: true
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: false
---

# üöß Installer Spark

## Installer Spark

Pour simplifier l'installation de Spark, nous avons con√ßu un script qui effectue cette installation de fa√ßon automatis√©e. Il est situ√© dans le dossier Python du dossier Raccourcis de votre bureau. Que vous souhaitiez adresser le logiciel Spark avec une syntaxe proche de R ou de Python, cette √©tape est n√©cessaire.

Cliquez sur AutoInstallSpark.bat afin d'effectuer l'installation.

Vous devrez d'abord choisir une version de Spark parmi la liste de celle disponible en saisissant son num√©ro. Je vous conseille la plus r√©cente.&#x20;

{% hint style="warning" %}
Vous devez attendre la fin du script et qu'il demande d'appuyer sur entr√©e pour terminer. Il ne faut pas fermer la fen√™tre du terminal ouvert pendant le script, cette op√©ration n'est pas tr√®s longue. Il suffit d'attendre la copie de l'ensemble des fichiers.
{% endhint %}

Ce script d√©compresse l'archive contenant Spark, copie les fichiers n√©cessaires √† son utilisation sous Windows, et param√®tres les variables d'environnement.

Vous pouvez tester l'installation avec la commande suivante dans un environnement anaconda par exemple (Miniconda Prompt)¬†:

```bash
spark-shell
```

### Installer les autres d√©pendances de Spark

Les fichiers Jar additionnels dont vous pouvez avoir besoin pour installer des librairies Spark peuvent √™tre obtenus dans le m√™me dossier que pour l'installation de SparklyR : dans S:\Spark\jar\_files. Vous trouverez ici les binaires √† copier, par exemple, celui qui permet d'ouvrir un fichier SAS avec Spark : spark-sas7bdat, dans diff√©rentes versions. De m√™me, vous pourrez installer parso avec la man≈ìuvre qui suit :

* Copiez le Jar de votre choix dans le dossier : "C:\Users\projet\_0\_p\_nom0000\AppData\Local\spark\spark-X.X.X-bin-hadoopX\jars"

## Acc√®s via R

### Utiliser SparkR

SparkR ne n√©cessite pas d'installation suppl√©mentaire √† proprement parler. Il suffit d'ins√©rer les deux lignes suivantes au d√©but de votre script, qui pr√©cisent o√π se situe la librairie SparkR et d√©marrent la session.

```r
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
sparkR.session(master = "local[*]", sparkConfig = list(spark.driver.memory = "2g"))
```

N'h√©sitez pas √† modifier ou ajouter des √©l√©ments √† la configuration dans la seconde ligne, afin de choisir des ressources adapt√©es √† votre traitement.

### Installer le package SparklyR

Si vous souhaitez utiliser Spark via une interface R pour soumettre vos instructions au cluster, vous pouvez utiliser la librairie SparklyR. Voici la marche √† suivre une fois que l'installation de Spark a √©t√© effectu√©e :

Proc√©dez d'abord √† l'installation du package SparklyR √† l'aide du d√©p√¥t, comme dans les chapitres pr√©c√©dents, puis chargez-le¬†:

```r
install.packages("sparklyr")
library("sparklyr")
```

Ensuite, connectez-vous au cluster :

```r
sc <- spark_connect(master="local")
```

Vous poss√©dez un serveur Spark adressable avec R pour votre application‚ÄØ!

## Acc√®s via Python

### Installer PySpark et les Notebook Jupyter

{% hint style="info" %}
Il est n√©cessaire de poss√©der un environnement anaconda dans une version compatible avec la version de Spark install√©e. Par exemple, **pour Spark 3.3.2, Python 3.9 est adapt√©, mais pas 3.11.** La matrice contenant les versions support√©es de [Python pour Spark est disponible ici.](https://community.cloudera.com/t5/Community-Articles/Spark-Python-Supportability-Matrix/ta-p/379144) L'extrait qui nous int√©resse est celui-ci :&#x20;
{% endhint %}

| Spark | Python minimum | Python maximum |
| :---: | :------------: | :------------: |
| 3.0.x |       3.4      |       3.8      |
| 3.1.x |       3.6      |       3.9      |
| 3.2.x |       3.6      |       3.9      |
| 3.3.x |       3.7      |      3.10      |
| 3.4.x |       3.7      |      3.11      |
| 3.5.x |       3.8      |      3.11      |

PySpark peut √™tre appel√© en choisissant un kernel adapt√© pour ex√©cuter votre notebook. Ouvrez un miniconda prompt dans la version de votre choix.&#x20;

<pre class="language-python"><code class="lang-python"><strong>conda create --name nom_env python --offline
</strong></code></pre>

Lancez et gardez ouvert le script pip-install-package et saissisez, dans votre miniconda prompt :

```python
pip install pyspark==3.3.2
pip install ipykernel --user
```

La version de votre paquet python Pyspark doit √™tre identique √† la version de Spark install√©e sur votre espace ! Puisqu'il s'agit d'un nouvel environnement d√©di√© aux notebook, ipykernel est √©galement n√©c√©ssaire.

Vous pouvez maintenant choisir le kernel adapt√© dans Visual Studio Code. Attention √† choisir le bon kernel, et que l'environnement en question poss√®de le package ipykernel qui permet d'ex√©cuter des notebooks.

En cas d'erreur avec py4j, vous pouvez ajouter deux cellules en entr√©e de script (avant la cr√©ation de l'environnement Spark !) :&#x20;

```python
import os 
import sys

os.environ['PYTHON_SPARK'] = sys.executable
os.environ['PYTHON_DRIVER_SPARK'] = sys.executable
```

