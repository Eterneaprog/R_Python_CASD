---
layout:
  title:
    visible: true
  description:
    visible: true
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: false
---

# üêç Utiliser Spark avec Python

## Configurer Spark en mode local

Dans le pr√©c√©dent chapitre, nous avons appris comment [installer un client pour utiliser Spark](6\_spark\_install.md). Nous allons maintenant r√©server des ressources afin de pr√©parer Spark pour notre traitement. Voici la marche √† suivre pour se connecter en mode local :

```python
from pyspark.sql import SparkSession

# Configuration des ressources
spark = SparkSession.builder \
    .appName("ExempleJointureSimple") \  # Nom de l'application Spark
    .config("spark.executor.memory", "8g") \  # M√©moire allou√©e par ex√©cuteur
    .config("spark.executor.cores", 4) \  # Coeurs par ex√©cuteur
    .config("spark.driver.memory", "4g") \  # M√©moire allou√©e au driver (processus principal)
    .config("spark.driver.cores", 2) \  # Coeurs allou√©s au driver
    .getOrCreate()
```

Voici un exemple simple de configuration Spark. Les ressources doivent √™tre proportionnelles √† la t√¢che effectu√©e et aux donn√©es trait√©es par votre application.

## Effectuer des traitements

Voyons d√©sormais comment utiliser ces outils pour effectuer des calculs. Il existe de nombreuses ressources, en particulier la [documentation officielle de Spark](https://spark.apache.org/docs/latest/quick-start.html). Nous allons ici voir des exemples fondamentaux :

* Comment ex√©cuter une requ√™te SQL
* Comment joindre deux tables avec Spark

Ces deux exemples seront trait√©s en PySpark.

Je ne pr√©senterai pas dans ce guide les utilisations avanc√©es de Spark (Gestion des graphs avec GraphX, Machine Learning avec MLlib ou encore le mode streaming). Ces utilisations sont tr√®s sp√©cifiques, mais font par ailleurs l'objet de documentation en ligne. Je fais mention de leur existence, car ces librairies sont tr√®s puissantes. Elles permettent des applications sur des volumes de donn√©es qui ne sont pas comparables avec les autres librairies disponibles sur le march√©.

### Faire une requ√™te SQL

Si vous d√©marrez un processus de conversion de code vers Spark, il est possible de ne pas imm√©diatement convertir l'ensemble du code. En effet, Spark est tout √† fait capable d'ex√©cuter du code SQL. Son moteur de calcul est moins optimis√© que lorsqu'il ex√©cute du code Spark. Cependant, cela reste une option int√©ressante en raison de son faible co√ªt de code, et de ses performances tout √† fait raisonnables.

On suppose ici aussi que l'environnement est initialis√©, cette fois, faisons une agr√©gation :

```python
# Charger une table Spark temporaire depuis un fichier Parquet :
df = spark.read.parquet("/chemin/vers/France.parquet")
df.createOrReplaceTempView("France")

# Ex√©cuter une requ√™te SQL
result = spark.sql("SELECT region, count(departement) FROM France GROUP BY region")

# Afficher le r√©sultat
result.show()
spark.stop()
```

J'ai ici imagin√© une table France, contenant les d√©partements et leur r√©gion d'appartenance. J'ai compt√© le nombre de d√©partements par r√©gion en utilisant un group by. De plus, il faut toujours fermer sa session avec spark.stop().

### Faire une jointure

Dans cet exemple, deux tables seront cr√©√©es, mais on pourrait les charger depuis des fichiers dans des Spark DataFrames directement.

<table><thead><tr><th width="76" align="center">id</th><th width="261" align="center">valeur1</th><th width="67" align="center">id</th><th align="center">valeur2</th><th data-hidden></th></tr></thead><tbody><tr><td align="center">1</td><td align="center">A</td><td align="center">2</td><td align="center">X</td><td></td></tr><tr><td align="center">2</td><td align="center">B</td><td align="center">3</td><td align="center">Y</td><td></td></tr><tr><td align="center">3</td><td align="center">C</td><td align="center">1</td><td align="center">Z</td><td></td></tr></tbody></table>

On transforme les DataFrames cr√©√©s en Spark DataFrames avec les noms de colonnes appropri√©s. Enfin, on performe une jointure sur les deux tables.

<table><thead><tr><th width="73" align="center">id</th><th width="324" align="center">valeur1</th><th align="center">valeur2</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">A</td><td align="center">Z</td></tr><tr><td align="center">2</td><td align="center">B</td><td align="center">X</td></tr><tr><td align="center">3</td><td align="center">C</td><td align="center">Y</td></tr></tbody></table>

Voyons comment effectuer cette manipulation en PySpark :

```python
# Cr√©er deux DataFrames
data1 = [(1, "A"), (2, "B"), (3, "C")]
data2 = [(1, "Z"), (2, "X"), (3, "Y")]

columns1 = ["id", "valeur1"]
columns2 = ["id", "valeur2"]

df1 = spark.createDataFrame(data1, columns1)
df2 = spark.createDataFrame(data2, columns2)

# Effectuer la jointure
result = df1.join(df2, "id", "inner")

# Afficher le r√©sultat
result.show()
spark.stop()
```

Dans les deux exemples, on charge d'abord les donn√©es, puis on effectue la jointure entre les deux DataFrames cr√©√©s. Enfin, on affiche les r√©sultats avant de fermer la session. M√™me s'il s'agit du mode local, la fermeture de la session est une bonne pratique qu'il faut appliquer syst√©matiquement √† la fin d'un code Spark pour lib√©rer les ressources.
