---
layout:
  title:
    visible: true
  description:
    visible: true
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: false
---

# ğŸš§ Installer Spark

## Les Ã©lÃ©ments nÃ©cessaires

### Installer Spark

Pour simplifier l'installation de Spark, nous avons conÃ§u un script qui effectue cette installation de faÃ§on automatisÃ©e. Il est situÃ© dans l'espace commun. Que vous souhaitiez adresser le logiciel Spark avec une syntaxe proche de R ou de Python, cette Ã©tape est nÃ©cessaire.

Cliquez sur AutoInstallSpark.bat afin d'effectuer l'installation.

**Attention :** Vous devez attendre la fin du script et qu'il demande d'appuyer sur entrÃ©e pour terminer. Il ne faut pas fermer la fenÃªtre du terminal ouvert pendant le script, cette opÃ©ration n'est pas trÃ¨s longue.

Ce script dÃ©compresse l'archive contenant Spark, copie les fichiers nÃ©cessaires Ã  son utilisation sous Windows, et paramÃ¨tres les variables d'environnement.

Vous pouvez tester l'installation avec la commande suivante dans un terminal de commande (cmd)Â :

```bash
spark-shell
```

### Installer le package SparklyR pour travailler avec SparkR dans Rstudio

Si vous souhaitez utiliser SparkR pour soumettre vos instructions au cluster, vous aurez besoin de la librairie SparklyR. Voici la marche Ã  suivre une fois que l'installation de Spark a Ã©tÃ© effectuÃ©e :

ProcÃ©dez d'abord Ã  l'installation du package SparklyR Ã  l'aide du dÃ©pÃ´t, comme dans les chapitres prÃ©cÃ©dents, puis chargez-leÂ :

```r
install.packages("sparklyr")
library("sparklyr")
```

Ensuite, connectez-vous au cluster :

```r
sc <- spark_connect(master="local")
```

Vous possÃ©dez un serveur Spark adressable avec R pour votre applicationâ€¯!

### Installer PySpark et adresser le cluster avec un Notebook Jupyter

PySpark est inclus nativement avec Spark. Une fois l'installation de Spark effectuÃ©e avec le script fourni, vous pouvez accÃ©der Ã  Pyspark dans votre application avec la commande :

```bash
pyspark
```

### Installer les autres dÃ©pendances de Spark

Les fichiers Jar additionnels dont vous pouvez avoir besoin pour installer des librairies Spark peuvent Ãªtre obtenus dans le mÃªme dossier que pour l'installation de SparklyR : dans S:\Spark\jar\_files. Vous trouverez ici les binaires Ã  copier, par exemple, celui qui permet d'ouvrir un fichier SAS avec Spark : spark-sas7bdat, dans diffÃ©rentes versions. De mÃªme, vous pourrez installer parso avec la manÅ“uvre qui suit :

* Copiez le Jar de votre choix dans le dossier : "C:\Users\projet\_0\_p\_nom0000\AppData\Local\spark\spark-3.3.2-bin-hadoop2\jars"

