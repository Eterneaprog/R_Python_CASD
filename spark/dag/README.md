# üë©‚Äçüè´ Optimiser Spark

Avant de s'int√©resser aux optimisations, il est tr√®s important de bien maitriser les notions de transformations et d'actions en Spark. Elles ont des implications fortes sur le code et son ex√©cution √† cause du ph√©nom√®ne "Lazy evaluation". Un rappel sur ces notions est disponible ici :&#x20;

[Rappel sur la diff√©rence entre actions et transformations](actions-ou-transformations.md#difference-entre-les-deux-notions)

De plus, les transformations sont r√©parties en deux types, celles qui mobilisent un sous-ensemble des donn√©es (narrrow), ou celles qui ont besoin de l'ensemble des donn√©es (wide). Il est important de pouvoir identifier de quel type est une transformation avant de l'ajouter √† son code. Un rappel sur la diff√©rence entre ces deux types de transformations, ainsi que des exemples sont disponibles ici :&#x20;

[Rappel sur les types de transformations Spark](actions-ou-transformations.md#transformations-narrow-contre-wide)

Spark embarque un module charg√© d'optimiser votre code et qui rend ce langage tr√®s performant : Catalyst. Comprendre le fonctionnement de Catalyst et des plans d'ex√©cutions aide grandement √† comprendre pourquoi les optimisations que nous allons appliquer par la suite sont, ou ne sont pas efficaces. Cependant, il est possible d'appliquer les bonnes pratiques sans saisir les d√©tails de ces notions.&#x20;

[Le fonctionnement de Catalyst et des DAG en Spark](les-plans-dexecutions-et-catalyst.md)

On peut appliquer une m√©thode afin d'optimiser son code et tester que ces optimisations sont efficaces :&#x20;

[Une m√©thodologie de l'optimisation](une-methode-pour-tester-son-optimisation.md)

Dans cette m√©thodologie, vous verrez comment d√©tecter ce qui rend le code lent, quelles sont les optimisations disponibles, et comment les appliquer. Entre autres : la configuration adapt√©e, le partitionnement, le cache.
