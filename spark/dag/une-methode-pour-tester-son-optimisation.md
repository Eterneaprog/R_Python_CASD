# üî¨ Une m√©thode pour tester son optimisation

Nous avons vu les techniques avanc√©es que Catalyst met en place pour optimiser les temps de calculs dans l'article pr√©c√©dent.  Voyons maintenant comment nous pouvons en tirer profit par un code optimal en tenant compte du comportement de ce module.&#x20;

### √âtape 1 : D√©tecter les engorgements via l'interface graphique

L'interface graphique Spark contient beaucoup d'informations que l'on peut utiliser afin de choisir son optimisation.

Par exemple, on peut regarder l'ad√©quation de la configuration au code utilis√© (les ressources m√©moires sont-elles suffisantes/insuffisantes ou surdimensionn√©es ?)

\[Image ressources]

On peut regarder √©galement s'il y a beaucoup de mouvements de donn√©es et des engorgements. Nous avons appris o√π lire un diagramme DAG dans l'article pr√©c√©dent. Ce que l'on recherche dans ce diagramme est les engorgements. C'est-√†-dire, les √©tapes qui provoquent un ralentissement en bloquant l'ensemble du code. Cela se mat√©rialise souvent par la pr√©sence d'un shuffle, qui sont souvent la cons√©quence d'une instruction de nature Wide :

<figure><img src="../../.gitbook/assets/example partition.png" alt=""><figcaption></figcaption></figure>

Ici, on voit que l'engorgement se fait au moment o√π l'on passe de l'√©tape 8 √† 9, car il y a lecture et √©criture de 58 Giga de donn√©es entre les deux √©tapes. C'est-√†-dire, transfert de donn√©es.&#x20;

On peut aussi regarder quelles sont les √©tapes qui consomment le plus de temps (de CPU et de RAM dans le tableau en dessous √©galement). Cet exemple d'Apache Spark montre les √©tapes du calcul de Pi :

<figure><img src="../../.gitbook/assets/summary.PNG" alt=""><figcaption></figcaption></figure>

On peut alors d√©terminer dans notre code ce qui est responsable de la dur√©e du traitement. On peut ensuite prendre une d√©cision adapt√©e en apportant des modifications.\
Il ne faut pas h√©siter √† explorer cette interface afin de trouver les probl√®mes.

### √âtape 2 : Utiliser une technique pour optimiser

Il existe diff√©rentes techniques afin d'optimiser son code, √† adapter selon la cause de l'engorgement d√©tect√©e √† l'√©tape 1. Parmi celles-ci, on trouve, entre autres :&#x20;

* [L'adaptation de la configuration](boite-a-outil-des-optimisations/choisir-une-configuration-spark.md) (efficace lors des saturations m√©moires)
* [√âliminer certaines instructions](boite-a-outil-des-optimisations/forcer-les-calculs.md) de son code pour laisser Catalyst optimiser mieux (efficace lorsque le code utilise des collect/compute et est donc tr√®s hach√©)
* [Le choix d'une strat√©gie de jointure alternative](boite-a-outil-des-optimisations/choisir-sa-strategie-de-jointure.md) (efficace lors d'une jointure)
* [L'utilisation d'un cache](boite-a-outil-des-optimisations/utiliser-le-cache-pour-ameliorer-ses-calculs.md) (efficace lors de la r√©utilisation d'une table)
* [Le partitionnement d'une table](boite-a-outil-des-optimisations/partitionner-une-table-selon-une-variable.md) selon une variable bien choisie (efficace lorsque l'on va utiliser une table par une variable pr√©cise par la suite).

### √âtape 3 : Comparer avec ou sans l'optimisation

Une fois votre strat√©gie d'optimisation d√©finie, vous pouvez effectuer un benchmark pour comparer si la strat√©gie d'optimisation a permis de r√©duire le temps de calcul. En utilisant le Spark UI, vous pouvez obtenir la dur√©e de traitement ou alors d√©finir dans votre code des balises de temps pour mesurer la dur√©e d'une fonction :&#x20;

{% tabs %}
{% tab title="PySpark" %}
```python
from pyspark.sql import SparkSession
import time

def my_spark_function():
    spark = SparkSession.builder.appName("mon_application").getOrCreate()
    
    # Transformations
    df = spark.read.csv("chemin/vers/votre/fichier.csv", header=True, inferSchema=True)
    result_df = df.groupBy("colonne").count()
    
    # Action
    result_df.show()

start_time = time.time()
my_spark_function()
end_time = time.time()
duration = end_time - start_time

# Afficher la dur√©e
print(f"La fonction Spark a mis {duration} secondes pour s'ex√©cuter.")
```
{% endtab %}

{% tab title="SparkR" %}
```r
# Chargement de la biblioth√®que SparkR
library(SparkR)

# D√©finition de la fonction Spark
my_spark_function <- function() {
  sparkR.session(master = "local[*]", appName = "mon_application")
  
  # Transformations
  df <- read.df("chemin/vers/votre/fichier.csv", source = "csv", header = "true", inferSchema = "true")
  result_df <- summarize(groupBy(df, df$colonne), count = n(df$colonne))
  
  # Action
  showDF(result_df)
  
  # Arr√™ter la session Spark
  sparkR.session.stop()
}

# Calcul de la dur√©e d'ex√©cution
start_time <- Sys.time()
my_spark_function()
end_time <- Sys.time()
duration <- as.numeric(difftime(end_time, start_time, units = "secs"))

# Afficher la dur√©e
cat(sprintf("La fonction Spark a mis %f secondes pour s'ex√©cuter.\n", duration))
```
{% endtab %}

{% tab title="SparklyR" %}
{% code overflow="wrap" fullWidth="false" %}
```r
library(sparklyr)
library(dplyr)

my_spark_function <- function() {
  spark <- spark_connect(master = "local", version = "3.0.2")
  
  # Traitement
  df <- spark_read_csv(spark, "chemin/vers/votre/fichier.csv", header = TRUE, infer_schema = TRUE)
  result_df <- df %>% group_by(colonne) %>% summarize(count = n())

  # Action
  spark_print(result_df)
}

# Calcul de la dur√©e d'ex√©cution
start_time <- Sys.time()
my_spark_function()
end_time <- Sys.time()
duration <- as.numeric(difftime(end_time, start_time, units = "secs"))

# Afficher la dur√©e
cat(sprintf("La fonction Spark a mis %f secondes pour s'ex√©cuter.\n", duration))
```
{% endcode %}
{% endtab %}
{% endtabs %}
