# üß© Partitionner une table selon une variable

## Partitionnement ?

Le partitionnement pour un dataframe Spark, c'est une fa√ßon de le ranger, de l'organiser, de le stocker. Cela ne change pas le contenu de la table ou les lignes, mais comment on va diviser cette table en sous-ensembles.&#x20;

On appelle chacun de ces sous-ensembles partition. Il peut y en avoir plus ou moins selon le crit√®re pour construire les sous-ensembles. On peut partitionner de fa√ßon purement math√©matique, en demandant un nombre fixe de partitions que l'on traitera de fa√ßon ind√©pendante, comme des batchs. Mais on peut aussi partitionner de fa√ßon 'm√©tier', afin de regrouper les donn√©es d'une mani√®re qui a du sens par rapport au traitement. Par exemple, diviser une table contenant des v√©hicules par marque. S'il y a 10 marques de voitures dans la table, on obtiendra 10 partitions √† traiter.

## Pourquoi partitionner ?

Partitionner, cela permet de tirer pleine puissance des architectures distribu√©es. Une bonne partition, cela permet de minimiser les mouvements de donn√©es lors des traitements. C'est donc un gain de performance tr√®s important.&#x20;

### Une r√®gle g√©n√©rale de partitionnement

> La r√®gle g√©n√©rale pour trouver les variables par lesquelles on doit partitionner est de regarder quelles variables sont utilis√©es pour faire des group by et des filtres √† plusieurs reprises.

Par exemple, dans l'exemple pr√©c√©dent sur les v√©hicules, si l'on va ensuite calculer la moyenne de prix des v√©hicules pour chaque marque, le temps de calcul va √™tre drastiquement r√©duit : il suffit d'appliquer la fonction moyenne √† chaque sous-ensemble. Cela signifie concr√®tement qu'au moment d'effectuer un group by (instruction de nature wide, donc couteuse, et qui engendre des mouvements de donn√©es), aucun mouvement ne se produira. En effet, la moyenne sera simplement appliqu√©e √† l'endroit o√π chaque partition est stock√©e (sur chaque worker).&#x20;

Observez ce sch√©ma qui r√©sume cet exemple :&#x20;

<figure><img src="../../../.gitbook/assets/partition.png" alt=""><figcaption></figcaption></figure>

L'utilisation d'une partition par la variable marque permet d'√©viter l'envoi des donn√©es entre les deux n≈ìuds puisque toutes les donn√©es du calcul sont d√©j√† accessibles par celui-ci ! Il n'y a donc pas de shuffle, qui est tr√®s couteux en temps et en ressources. Admettons que je souhaite ensuite calculer l'√©cart type, puis le maximum et minimum de chaque marque, on effectue des gains de temps pour chacune de ces op√©rations. Cela peut donc se r√©v√©ler tr√®s rentable. De plus, m√™me dans le cas d'un calcul de moyenne g√©n√©ral, ce choix de partition n'est pas p√©nalisant ! Ici, on peut donc parler de partitionnement 'naturel'.

Une partition peut √™tre vue comme pr√©-traiter les donn√©es avec un group-by. Cela peut √™tre payant si on va de toute fa√ßon appliquer un tel group by √† plusieurs reprises.&#x20;

### Les cas d√©favorables

Cependant, il faut aussi √™tre prudent lorsque l'on partitionne. Si on choisit une partition inadapt√©e au traitement, cela peut engendrer encore plus de mouvements de donn√©es ! En effet, une table partitionn√©e avec beaucoup de partitions est plus longue √† interroger lorsque l'on mobilise des donn√©es dans beaucoup de partitions. &#x20;

Voici le r√©sultat d'un benchmark que nous avons effectu√© sur une table :

<table><thead><tr><th width="409">Requ√™te / Partitions </th><th width="113" align="center">2</th><th width="103" align="center">5</th><th align="center">1000</th></tr></thead><tbody><tr><td>Requ√™te sur une colonne qui a servi au partitionnement</td><td align="center">74,50%</td><td align="center">46,30%</td><td align="center">16,66%</td></tr><tr><td>Requ√™te vers une autre colonne</td><td align="center">89,51%</td><td align="center">191,01%</td><td align="center">556,99%</td></tr><tr><td>Select distinct(*)</td><td align="center">136,79%</td><td align="center">163,68%</td><td align="center">1194,88%</td></tr></tbody></table>

On voit que le gain de temps est r√©el sur les requ√™tes avec des colonnes qui ont servi au partitionnement. Cependant, cela fait augmenter le temps de calcul sur les requ√™tes qui mobilisent plusieurs partitions. La meilleure solution est donc souvent de choisir une partition avec un nombre limit√© de valeurs afin d'√©viter l'√©cueil principal : faire trop de partitions.

Il n'existe pas de partitionnement id√©al ou par d√©faut, il faut toujours r√©fl√©chir √† l'utilisation des donn√©es pour d√©terminer le partitionnement √† utiliser.

## Comment partitionner ?

### Avec Sparklyr

Partitionner un dataframe spark avec Sparklyr est tr√®s simple, d'abord par nombre de partitions :

```r
sdf_repartition(my_dataframe, partitions = 10, partition_by = NULL) 
```

ou par variable(s) :&#x20;

```r
sdf_repartition(my_dataframe, partitions = NULL, partition_by = "my_variable") 
```

On peut aussi partitionner avec plusieurs variables, en rempla√ßant la chaine de caract√®re par un vecteur.

### Avec PySpark

Partitionner un dataframe spark avec PySpark peut se faire de la fa√ßon suivante, d'abord par nombre de partitions :

```python
my_dataframe.repartition(10)
```

ou par variable(s) :&#x20;

```python
my_dataframe.repartition("my_variable")
```

On peut aussi partitionner avec plusieurs variables, en rempla√ßant la chaine de caract√®re par un vecteur.

## Exemples de partitionnements par variables pertinents :

Si on acc√®de √† des donn√©es g√©ographiques, il est int√©ressant de partitionner par r√©gion/d√©partement/ville. On va gagner beaucoup de temps lors de la lecture par la suite.

Imaginons que l'on √©tudie une table sur des individus, contenant des donn√©es de sant√© ou des donn√©es √©conomiques, selon le contexte d'utilisation et selon la mani√®re m√™me dont la table est construite, le partitionnement peut totalement changer.

Si dans cette table, plusieurs lignes correspondent √† des informations sur le m√™me individu, et donc qu'il n'est pas unique, on peut partitionner avec l'identifiant de l'individu. C'est particuli√®rement efficace si on va effectuer des traitements par individus par la suite.&#x20;

Si dans cette table, chaque individu est unique, et que l'on va chercher √† faire des groupes, le partitionnement par individu est inutile. En effet, on ne fera pas de regroupement puisqu'un individu correspond √† une ligne dans la table. Par contre, on peut partitionner par la variable Sexe et Age par exemple.

La documentation officielle sur le [repartitionnement d'une table](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.repartition.html)

## La r√®gle g√©n√©rale si on ne peut pas partitionner par variable

En r√®gle g√©n√©rale, le nombre de partitions d'une table doit √™tre contenu entre :&#x20;

Au minimum : 2 x le nombre de c≈ìurs choisis pour l‚Äôapplication

Au maximum : au moins 100ms de temps d'ex√©cution par partition, au dela, c'est que la table est trop partitionn√©e, et donc on perd du temps lors des op√©rations qui n√©c√©ssitent un shuffling.

Cependant, cette r√®gle g√©n√©rale n'est pas vraiment la meilleure fa√ßon de proc√©der. Elle ne tient pas compte du contexte sp√©cifique m√©tier dans lequel la table sera utilis√©e. Souvent, il est bien plus int√©ressant d'utiliser ce contexte pour partitionner de mani√®re intelligente.
