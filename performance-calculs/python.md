---
layout:
  title:
    visible: true
  description:
    visible: true
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: false
---

# üêç Parall√©liser son code Python

### Parall√©liser un calcul avec Python

Le calcul utilisant plusieurs c≈ìurs est possible en Python et en R. Un certain nombre de traitements natifs ou accessibles via des paquets sont d'ailleurs naturellement parall√©lis√©s dans ces langages. Voici un exemple de calcul multic≈ìur utilisant des processus en Python. Il ne s'agit pas de la seule fa√ßon de faire. Le principe de fonctionnement est souvent identique¬†:

<figure><img src="../chapters/images/parallel.png" alt=""><figcaption><p>Le multi processing permet de gagner du temps d'ex√©cution</p></figcaption></figure>

Il s'agit de distribuer la charge de travail sur plusieurs c≈ìurs/machines afin d'assurer une vitesse de calcul optimale plut√¥t que de faire faire les op√©rations par un seul c≈ìur. En effet, on fait donc plusieurs calculs en m√™me temps plut√¥t que les uns apr√®s les autres.

G√©n√©ralement, un processus central est responsable d'invoquer les autres et de r√©cup√©rer leurs r√©sultats. De cette mani√®re, on divise la charge des diff√©rents c≈ìurs du nombre de c≈ìurs utilis√©s pour les calculs.

**Attention¬†:** L'ensemble des calculs ne se pr√™tent pas au calcul distribu√©, en particulier si les calculs √† effectuer sur les diff√©rentes op√©rations sont fortement li√©s. Ce paradigme est particuli√®rement efficace et simple √† mettre en place sur les traitements s√©quentiels ind√©pendants.

```python
import multiprocessing

# Fonction qui sera ex√©cut√©e dans chaque processus ind√©pendant
def worker_function(worker_id, data):
    print(f"Worker {worker_id} traite {data}")
    # Vous pouvez ins√©rer ici vos traitements

if __name__ == "__main__":
    # Donn√©es √† traiter (peut √™tre une liste, un dictionnaire, etc.)
    data_to_process = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

    # Cr√©er un pool de processus avec multiprocessing.Pool
    num_processes = 4  # Utilise 4 coeurs pour effectuer le traitement
    pool = multiprocessing.Pool(processes=num_processes)

    # Mapper la fonction de travail sur les donn√©es en parall√®le
    # Chaque processus ex√©cute la worker_function avec une partie des donn√©es
    results = []
    for i, data_chunk in enumerate(data_to_process):
        result = pool.apply_async(worker_function, (i, data_chunk))
        results.append(result)

    # Attendre la fin de tous les processus
    pool.close()
    pool.join()

    # R√©cup√©rer les r√©sultats si n√©cessaire
    final_output = [result.get() for result in results]

    # Traitement final avec les r√©sultats si n√©cessaire
    # ...

    print("Tous les processus sont termin√©s.")

```

Dans ce code, on ex√©cute un traitement sur les donn√©es de fa√ßon parall√®le avec 4 processus. On commence par d√©finir le traitement que chaque processus va effectuer. Puis, la partie du code qui suite l'instruction `if __name__ == "__main__"` correspond au processus central sur le sch√©ma pr√©c√©dent. Il commence par invoquer les autres processus. Puis, il leur assigne une t√¢che √† effectuer et enregistre les r√©sultats. Une fois ces traitements effectu√©s, les processus sont termin√©s et on obtient le r√©sultat final que l'on peut retraiter dans le processus central (ou non si ce n'est pas n√©cessaire).

**Attention¬†:** Les ressources m√©moires associ√©es √† chaque processus sont partag√©es dans cet exemple ! Cela veut dire que l'utilisation massive de processus peut provoquer une saturation de la m√©moire RAM. Chaque processus peut √™tre amen√© √† utiliser des ressources m√©moires dans son traitement, et donc augmenter fortement la consommation par rapport √† un traitement monoprocesseur.
