# ğŸ˜ Gestion des donnÃ©es avec HDFS

Le systÃ¨me de fichier distribuÃ© distribuÃ© HDFS doit Ãªtre manipulÃ© en ligne de commandes contrairement Ã  l'explorateur de fichier. Cependant, le nombre de commandes Ã  connaÃ®tre est trÃ¨s limitÃ© et elles sont simples. Ces commandes peuvent Ãªtre utilisÃ©es depuis n'importe quel invitÃ© de commande (cmd dans la barre de recherche).

Pour l'ensemble de ces commandes, il est nÃ©cessaire de possÃ©der l'adresse du NameNode de votre cluster puisque c'est ici que HDFS s'attend Ã  recevoir les donnÃ©es. Vous pouvez le trouver dans les documents spÃ©cifiques Ã  votre cluster dans l'espace commun.

## Ajouter des donnÃ©es Ã  HDFS

Il suffit d'utiliser l'instruction -put et de prÃ©ciser les chemins de fichiers :

```bash
hdfs dfs -put /chemin/vers/le/fichier_source.csv /nom_destination.csv)
```

## RÃ©cupÃ©rer des donnÃ©es depuis HDFS

Il s'agit de la mÃªme commande avec l'instruction -get :

```bash
hdfs dfs -get /chemin-sur-hdfs/fichier.csv /chemin-systeme-local.csv
```

## Utiliser un fichier stockÃ© dans HDFS pour un traitement Spark avec Sparklyr

Lire un CSV ou un fichier parquet peut Ãªtre fait avec les commandes suivantes (il faut avoir dÃ©fini un [spark\_connect](sparklyr-en-mode-cluster.md#adapter-la-configuration) au prÃ©alable) :&#x20;

```r
spark_read_csv(sc, name = "nom_table", path = "hdfs://adresseNameNode:Port/fichier.csv")
```

```r
spark_read_parquet(sc, name = "nom_table", path = "hdfs://adresseNameNode:Port/fichier.parquet")
```
