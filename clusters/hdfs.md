# üêò Gestion des donn√©es avec HDFS

Le syst√®me de fichier distribu√© distribu√© HDFS peut √™tre manipul√© en ligne de commandes ou en interface graphique, comme l'explorateur de fichiers. Le nombre de commandes √† conna√Ætre est tr√®s limit√© et elles sont simples. Ces commandes peuvent √™tre utilis√©es depuis n'importe quel invit√© de commande (cmd dans la barre de recherche).&#x20;

L'organisation du syst√®me dans votre bulle est la suivante :&#x20;

<figure><img src="../.gitbook/assets/HDFS (1).png" alt=""><figcaption><p>Dans la bulle √©quip√©e d'un cluster, on peut persister ses donn√©es avec le syst√®me de fichier local ou HDFS !</p></figcaption></figure>

On voit que l'on peut d'abord monter des donn√©es du syst√®me de fichier local vers HDFS avec l'instruction -put. Puis, utiliser ces donn√©es avec la commande spark\_read\_csv qui permet de lire depuis un syst√®me de fichier distribu√©. \
\
Une fois le calcul achev√©, on peut persister notre r√©sultat avec spark\_write\_csv sur HDFS. Enfin, s'il est n√©cessaire de persister ces donn√©es sur le syst√®me de fichier local, on peut les t√©l√©charger avec l'interface graphique, ou avec hdfs dfs -get !

## Ajouter des donn√©es √† HDFS depuis le syst√®me de fichier local

Il suffit d'utiliser l'instruction -put et de pr√©ciser les chemins de fichiers :

```bash
hdfs dfs -put /chemin/vers/le/fichier_source.csv /nom_destination.csv)
```

## Ajouter des donn√©es sur le syst√®me local depuis HDFS&#x20;

Il s'agit de la m√™me commande avec l'instruction -get :

```bash
hdfs dfs -get /chemin-sur-hdfs/fichier.csv /chemin-systeme-local.csv
```

## Ajouter un fichier √† la RAM du cluster depuis HDFS&#x20;

Lire un CSV ou un fichier parquet peut √™tre fait avec les commandes suivantes (il faut avoir d√©fini un [spark\_connect](pyspark.md) au pr√©alable) :&#x20;

```r
spark_read_csv(sc, name = "nom_table", path = "hdfs://chemin/fichier.csv")
```

```r
spark_read_parquet(sc, name = "nom_table", path = "hdfs://chemin/fichier.parquet")
```

## Ajouter des donn√©es dans HDFS depuis la RAM du cluster

√âcrire un CSV ou un fichier parquet peut √™tre fait avec les commandes suivantes (il faut avoir d√©fini un [spark\_connect](pyspark.md) au pr√©alable) :&#x20;

```r
spark_write_csv(sc, spark_dataframe, path = "hdfs://chemin/fichier.csv")
```

```r
spark_write_parquet(sc, spark_dataframe, path = "hdfs://chemin/fichier.parquet")
```
